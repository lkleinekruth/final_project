{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"progan.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNZQUkB3rlSK2DvIVs8BhLa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OylPqQgRhYZf"},"source":["# Implementing a Progressive GAN\n","\n","Mara Rehmer and Linus kleine Kruthaup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Urf8yCmAhW9Z","executionInfo":{"status":"ok","timestamp":1617796349004,"user_tz":-120,"elapsed":6190,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}},"outputId":"3c09837c-8ef3-4205-d16e-8a1862711c6a"},"source":["!pip install MTCNN"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting MTCNN\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 6.1MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from MTCNN) (4.1.2.30)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from MTCNN) (2.4.3)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->MTCNN) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->MTCNN) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->MTCNN) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->MTCNN) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->MTCNN) (1.15.0)\n","Installing collected packages: MTCNN\n","Successfully installed MTCNN-0.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XZfz_bkhjK1","executionInfo":{"status":"ok","timestamp":1617798766402,"user_tz":-120,"elapsed":516,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["import tensorflow as tf\n","import numpy as np\n","from PIL import Image \n","import os\n","from mtcnn.mtcnn import MTCNN\n","import matplotlib.pyplot as plt \n","from keras.layers import Add, Conv2D, Layer, LeakyReLU, Reshape, Dense, AveragePooling2D, UpSampling2D, Flatten\n","from keras import backend, Sequential, Model, Input\n","from keras.initializers import RandomNormal\n","from keras.optimizers import Adam\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SueHuOhrh3iv"},"source":["## 1 Preparing the data\n","\n","The dataset we chose for our ProGAN to train on is CelebA (https://www.kaggle.com/jessicali9530/celeba-dataset). It consists of 202599 pictures of celebrity faces. The original dataset additionally holds information about different features of the face displayed in an image e.g. if the person is Bald or wears glasses. For our purposes we are only interested in the images though.\n","\n","As a starting point we used the imgalignceleba version of the dataset which already cropped and alligned all of the imagest to the same dimensions. But this preprocessing is not sufficient for the task at hand. We additionally need to extract only the faces of each picture, since we are not interested in the backround. \n","\n","In order to extract the faces we used a pre-trained Neural Network called MTCNN to perform face detection on the images (https://github.com/ipazc/mtcnn). The models detect_faces function takes as input an array of pixels and returns a dictionary of coordinates for feature points like the nose, eyes and mouth, the confidence level of the network for the image at hand and a bounding box with x, y, width and height informations which can be used to cropp the face of the image.\n","\n","Additionally we will only work with a subset of the original dataset for lower runtimes.  \n","\n","The following code chunk takes quite some time to execute so the resulting compressed file is available on our github so that it can be reused more easily. "]},{"cell_type":"code","metadata":{"id":"wj0ga181VNRE"},"source":["!unzip img_align_celeba.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAVLsy-Kh4LD","executionInfo":{"status":"ok","timestamp":1617783597308,"user_tz":-120,"elapsed":674,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def load_image(filename):\n","    with Image.open(filename) as image:\n","        # converts the Image object into array\n","        image = image.convert('RGB')\n","    return image"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVspYQ7-okHu"},"source":["def crop_faces(model, directory, n_images, crop_size = (128, 128)):\n","    faces = list()\n","    for filename in os.listdir(directory):\n","        image = load_image(directory + filename)\n","        face_data = model.detect_faces(np.asarray(image))\n","        if face_data != []:\n","            x, y, width, height = face_data[0]['box']\n","            image_cropped = image.crop((x, y, x+width, y+height)).resize(crop_size)\n","            faces.append(np.asarray(image_cropped))\n","            print(len(faces))\n","        if len(faces) >= n_images:\n","            break\n","    return faces\n","\n","mtcnn = MTCNN()\n","np.savez_compressed('img_align_celeba_128.npz', crop_faces(mtcnn, '/content/img_align_celeba/', 10000))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdPUfaSRr1ju"},"source":["Printing a few of the resulting images to see if everything functioned properly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"89EuZFd4qBgP","executionInfo":{"status":"error","timestamp":1617796354212,"user_tz":-120,"elapsed":506,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}},"outputId":"c50d10cb-0d58-496e-ad9d-9d0369fffdc4"},"source":["def plot_faces(faces, n):\n","    for i in range(n*n):\n","        pyplot.subplot(n, n, 1 + i)\n","        pyplot.axis('off')\n","        pyplot.imshow(faces[i])\n","    pyplot.show()\n","\n","data = np.load('img_align_celeba_128.npz')\n","plot_faces(data['arr_0'],5)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-25ce51fb8a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img_align_celeba_128.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplot_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img_align_celeba_128.npz'"]}]},{"cell_type":"markdown","metadata":{"id":"n-Bdar9B4xT3"},"source":["## 2 Model\n","\n","The general model architecture of a ProGAN features a discriminator and a generator similar to the architecture of a basic GAN. The core idea of a ProGAN however is to train the model on pictures of incrementaly increasing sizes. This is achieved by first training a model with a small origin resolution e.g. 4x4. Then we slowly fade in a new model with a higher resolution e.g. 8x8 with the help of a Custom Layer. \n","\n","### 2.1 Custom Layers\n","\n","#### 2.1.1 Weighted Sum\n","\n","This custom Layer is an extension to the Add merge Layer. It is used to combine the activations from two input layers e.g. two input paths in a discriminator or two output paths in a generator model. With its alpha variable the influence of either of the two paths can be controlled. In the training call we can then update the alpha of each model call that features the WeightedSum Layer dependent on the training step we are currently in. "]},{"cell_type":"code","metadata":{"id":"gzW21ooN4wJY","executionInfo":{"status":"ok","timestamp":1617796364298,"user_tz":-120,"elapsed":387,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["class WeightedSum(Add):\n","    # init with default value\n","    def __init__(self, alpha=0.0, **kwargs):\n","        super(WeightedSum, self).__init__(**kwargs)\n","        self.alpha = backend.variable(alpha, name='ws_alpha')\n","\n","    # output a weighted sum of inputs\n","    def _merge_function(self, inputs):\n","        # only supports a weighted sum of two inputs\n","        assert (len(inputs) == 2)\n","        # ((1-a) * input1) + (a * input2)\n","        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n","        return output"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HScENcRBEPOb"},"source":["#### 2.1.2 Pixel Normalization\n","\n","In order to avoid a scenario for the training of the ProGAN where the magnitudes in the generator and discriminator get too large as a result of the competing loss functions the ProGAN uses a Normalization Layer that normalizes each pixel in the activation map to unit length. Pixel Normalization is only applied to the generator.\n","\n","The formula for the normalization is defined as:\n","\n","$ b_(x,y) = a_(x,y) / \\sqrt{\\frac{1}{N} \\sum_{j=0}^{N-1}(a^j_{x,y})^2 + \\epsilon}$, where ${\\epsilon}$ is a small constant to deal with 0 means,\n","N the number of feature maps, $a_{x,y}$ the original and $b_{x,y}$ the normalized feature vector in pixel $(x,y)$, respectively."]},{"cell_type":"code","metadata":{"id":"EnCsQ4T6EXmN","executionInfo":{"status":"ok","timestamp":1617796366186,"user_tz":-120,"elapsed":1089,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["class PixelNormalization(Layer):\n","    def __init__(self, **kwargs):\n","        super(PixelNormalization, self).__init__(**kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        return inputs / backend.sqrt(backend.mean(inputs ** 2.0, axis=-1, keepdims=True) + 1.0e-8)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyiXuP079FGV"},"source":["#### 2.1.3 Minibatch Standard deviation\n","\n","GANs often struggle to capture all the variation of a training dataset. Minibatch Standard devation tries to improve on this by computing feature statistics of a minibatches activations. More precisely it is implemented by first computing the mean of the standard deviation for each feature in each spatial location over the minibatch. The resulting constant feature map is then concatenated to the activation maps."]},{"cell_type":"code","metadata":{"id":"mmrFWzgmzl5S","executionInfo":{"status":"ok","timestamp":1617796367519,"user_tz":-120,"elapsed":424,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["class MinibatchStdev(Layer):\n","    def __init__(self, **kwargs):\n","        super(MinibatchStdev, self).__init__(**kwargs)\n","\n","    def call(self, inputs, **kwargs):\n","        # Calculate stdev across all feature maps\n","        square_diffs = backend.square(inputs - backend.mean(inputs, axis=0, keepdims=True))\n","        var = backend.men(square_diffs, axis=0, keepdims=True) + 1e-8\n","        stdev = backend.sqrt(var)\n","        mean_stdev = backend.mean(stdev, keepdims=True)\n","        shape = backend.shape(inputs)\n","        output = backend.tile(mean_stdev, (shape[0], shape[1], shape[2], 1))\n","        return backend.concatenate([inputs, output], axis=1)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gr3iAggBLwBf"},"source":["#### 2.1.4 Equalized Learning rate\n","\n","Instead of carefully initializing the networks weights, the paper uses an equalized learning rate. This scales the weights at each layer with a constant.\n","This is performed during training in order to keep the networks weights at a similar scale during training.\n","\n","The weights are scaled as follows:\n","\n","$\\hat{w_i} = w_i / c$, where $w_i$ are the weights and c is the per-layer normalization constant from He's initializer\n","\n","The equalized learning rate can be implemented via a custom Conv2D layer.\n"]},{"cell_type":"code","metadata":{"id":"AIrRq9q9LKjx","executionInfo":{"status":"ok","timestamp":1617797667987,"user_tz":-120,"elapsed":655,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["class EqualizedConv2D(Conv2D):\n","    def __init__(self, *args, **kwargs):\n","        self.scale = 1.0\n","        super(EqualizedConv2D, self).__init__(*args, **kwargs)\n","\n","    def build(self, input_shape):\n","        fan_in = np.prod(input_shape[1:-1])\n","        self.scale = np.sqrt(2/fan_in)\n","        return super(EqualizedConv2D, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        outputs = backend.conv2d(inputs, self.kernel*self.scale, strides=self.strides, padding=self.padding, data_format=self.data_format, dilation_rate=self.dilation_rate)\n","        outputs = backend.bias_add( outputs, self.bias, data_format=self.data_format)\n","        if self.activation is not None:\n","            return self.activation(outputs)\n","        return outputs"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4v8W_SdPQXX2"},"source":["### 2.2 Loss\n","\n","The paper uses Wasserstein loss but also mentions least squared error"]},{"cell_type":"code","metadata":{"id":"qhDk5DGJPLl5","executionInfo":{"status":"ok","timestamp":1617797850441,"user_tz":-120,"elapsed":420,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def wasserstein_loss(y_true, y_pred):\n","    return backend.mean(y_true * y_pred)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9j-a7NcMRDUD"},"source":["### 2.3 Discriminator"]},{"cell_type":"code","metadata":{"id":"4uqC-mD8RKp5","executionInfo":{"status":"ok","timestamp":1617798587671,"user_tz":-120,"elapsed":420,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def add_discriminator_block(old_model, n_input_layers=3, filters=128):\n","    # weight initialization\n","    init = RandomNormal(stddev=1.0)\n","    # get shape of existing model\n","    in_shape = list(old_model.input.shape)\n","    # define new input shape as double the size\n","    input_shape = (in_shape[-2] * 2, in_shape[-2] * 2, in_shape[-1])\n","    in_image = Input(shape=input_shape)\n","    # define new input processing layer\n","    d = EqualizedConv2D(filters, (1, 1), padding='same', kernel_initializer=init)(in_image)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # define new block\n","    d = EqualizedConv2D(filters, (3, 3), padding='same', kernel_initializer=init)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    d = EqualizedConv2D(filters, (3, 3), padding='same', kernel_initializer=init)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    d = AveragePooling2D()(d)\n","    block_new = d\n","    # skip the input, 1x1 and activation for the old model\n","    for i in range(n_input_layers, len(old_model.layers)):\n","        d = old_model.layers[i](d)\n","    # define straight-through model\n","    model1 = Model(in_image, d)\n","    # compile model\n","    model1.compile(loss=wasserstein_loss, optimizer=Adam(lr=PARAMETERS.learning_rate,\n","                                                         beta_1=PARAMETERS.adam_beta1,\n","                                                         beta_2=PARAMETERS.adam_beta2,\n","                                                         epsilon=PARAMETERS.adam_epsilon))\n","    # downsample the new larger image\n","    downsample = AveragePooling2D()(in_image)\n","    # connect old input processing to downsampled new input\n","    block_old = old_model.layers[1](downsample)\n","    block_old = old_model.layers[2](block_old)\n","    # fade in output of old model input layer with new input\n","    d = WeightedSum()([block_old, block_new])\n","    # skip the input, 1x1 and activation for the old model\n","    for i in range(n_input_layers, len(old_model.layers)):\n","        d = old_model.layers[i](d)\n","    # define straight-through model\n","    model2 = Model(in_image, d)\n","    # compile model\n","    model2.compile(loss=wasserstein_loss, optimizer=Adam(lr=PARAMETERS.learning_rate,\n","                                                         beta_1=PARAMETERS.adam_beta1,\n","                                                         beta_2=PARAMETERS.adam_beta2,\n","                                                         epsilon=PARAMETERS.adam_epsilon))\n","    return [model1, model2]\n","\n","\n","# define the discriminator models for each image resolution\n","def define_discriminator(n_blocks, input_shape=(4, 4, 3)):\n","    # weight initialization\n","    init = RandomNormal(stddev=1.0)\n","    model_list = list()\n","    # base model input\n","    in_image = Input(shape=input_shape)\n","    # conv 1x1\n","    d = EqualizedConv2D(128, (1, 1), padding='same', kernel_initializer=init)(in_image)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # conv 3x3 (output block)\n","    d = MinibatchStdev()(d)\n","    d = EqualizedConv2D(128, (3, 3), padding='same', kernel_initializer=init)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # conv 4x4\n","    d = EqualizedConv2D(128, (4, 4), padding='same', kernel_initializer=init)(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # dense output layer\n","    d = Flatten()(d)\n","    out_class = Dense(1)(d)\n","    # define model\n","    model = Model(in_image, out_class)\n","    # compile model\n","    model.compile(loss=wasserstein_loss, optimizer=Adam(lr=PARAMETERS.learning_rate,\n","                                                        beta_1=PARAMETERS.adam_beta1,\n","                                                        beta_2=PARAMETERS.adam_beta2,\n","                                                        epsilon=PARAMETERS.adam_epsilon))\n","    # store model\n","    model_list.append([model, model])\n","    # create submodels\n","    for i in range(1, n_blocks):\n","        filters = 2**(10-i)\n","        if filters > 512:\n","            filters = 512\n","        if filters < 16:\n","            filters = 16\n","        # get prior model without the fade-on\n","        old_model = model_list[i - 1][0]\n","        # create new model for next resolution\n","        models = add_discriminator_block(old_model)\n","        # store model\n","        model_list.append(models)\n","    return model_list"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAqyH2G6S3SW"},"source":["### 2.4 Generator"]},{"cell_type":"code","metadata":{"id":"DIKIaz7NS0Vm","executionInfo":{"status":"ok","timestamp":1617798590237,"user_tz":-120,"elapsed":428,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["# add a generator block\n","def add_generator_block(old_model, filters=256):\n","    # weight initialization\n","    init = RandomNormal(stddev=1.0)\n","    # get the end of the last block\n","    block_end = old_model.layers[-2].output\n","    # upsample, and define new block\n","    upsampling = UpSampling2D()(block_end)\n","    g = EqualizedConv2D(filters, (3, 3), padding='same', kernel_initializer=init)(upsampling)\n","    g = PixelNormalization()(g)\n","    g = LeakyReLU(alpha=0.2)(g)\n","    g = EqualizedConv2D(filters, (3, 3), padding='same', kernel_initializer=init)(g)\n","    g = PixelNormalization()(g)\n","    g = LeakyReLU(alpha=0.2)(g)\n","    # add new output layer\n","    out_image = EqualizedConv2D(3, (1, 1), padding='same', kernel_initializer=init)(g)\n","    model1 = Model(old_model.input, out_image)\n","    # get the output layer from old model\n","    out_old = old_model.layers[-1]\n","    # connect the upsampling to the old output layer\n","    out_image2 = out_old(upsampling)\n","    # define new output image as the weighted sum of the old and new models\n","    merged = WeightedSum()([out_image2, out_image])\n","    model2 = Model(old_model.input, merged)\n","    return [model1, model2]\n","\n","\n","# define generator models\n","def define_generator(latent_dim, n_blocks, in_dim=4):\n","    # weight initialization\n","    init = RandomNormal(stddev=1.0)\n","    model_list = list()\n","    # base model latent input\n","    in_latent = Input(shape=(latent_dim,))\n","    # linear scale up to activation maps\n","    g = Dense(128 * in_dim * in_dim, kernel_initializer=init)(in_latent)\n","    g = Reshape((in_dim, in_dim, 128))(g)\n","    # conv 4x4, input block\n","    # noinspection DuplicatedCode\n","    g = EqualizedConv2D(128, (3, 3), padding='same', kernel_initializer=init)(g)\n","    g = PixelNormalization()(g)\n","    g = LeakyReLU(alpha=0.2)(g)\n","    # conv 3x3\n","    g = EqualizedConv2D(128, (3, 3), padding='same', kernel_initializer=init)(g)\n","    g = PixelNormalization()(g)\n","    g = LeakyReLU(alpha=0.2)(g)\n","    # conv 1x1, output block\n","    out_image = EqualizedConv2D(3, (1, 1), padding='same', kernel_initializer=init)(g)\n","    model = Model(in_latent, out_image)\n","    # store model\n","    model_list.append([model, model])\n","    # create submodels\n","    for i in range(1, n_blocks):\n","        filters = 2**(10-i)\n","        if filters > 512:\n","            filters = 512\n","        if filters < 16:\n","            filters = 16\n","        # get prior model without the fade-on\n","        old_model = model_list[i - 1][0]\n","        # create new model for next resolution\n","        models = add_generator_block(old_model, gpus=gpus)\n","        # store model\n","        model_list.append(models)\n","    return model_list"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T73Kp9t1Tjb6"},"source":["### 2.5 Composite Model"]},{"cell_type":"code","metadata":{"id":"XvgYJlWlTlAy","executionInfo":{"status":"ok","timestamp":1617798591420,"user_tz":-120,"elapsed":422,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["# define composite models for training generators via discriminators\n","def define_composite(discriminators, generators):\n","    model_list = list()\n","    # create composite models\n","    for i in range(len(discriminators)):\n","        g_models, d_models = generators[i], discriminators[i]\n","        # straight-through model\n","        d_models[0].trainable = False\n","\n","        model1 = Sequential()\n","        model1.add(g_models[0])\n","        model1.add(d_models[0])\n","        model1.compile(loss=wasserstein_loss, optimizer=Adam(lr=PARAMETERS.learning_rate,\n","                                                             beta_1=PARAMETERS.adam_beta1,\n","                                                             beta_2=PARAMETERS.adam_beta2,\n","                                                             epsilon=PARAMETERS.adam_epsilon))\n","        # fade-in model\n","        d_models[1].trainable = False\n","\n","        model2 = Sequential()\n","        model2.add(g_models[1])\n","        model2.add(d_models[1])\n","        model2.compile(loss=wasserstein_loss, optimizer=Adam(lr=PARAMETERS.learning_rate,\n","                                                             beta_1=PARAMETERS.adam_beta1,\n","                                                             beta_2=PARAMETERS.adam_beta2,\n","                                                             epsilon=PARAMETERS.adam_epsilon))\n","        # store\n","        model_list.append([model1, model2])\n","    return model_list"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN4CcqN-Uv79"},"source":["## 3 Training\n","\n","### 3.1 Helper functions"]},{"cell_type":"code","metadata":{"id":"4jZgFcNsT3HE","executionInfo":{"status":"ok","timestamp":1617798878494,"user_tz":-120,"elapsed":390,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def deprocess(x):\n","    x = np.clip(x, -1, 1)\n","    return (x + 1) / 2.0"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVBk0M8OU9NV","executionInfo":{"status":"ok","timestamp":1617798900396,"user_tz":-120,"elapsed":437,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def summarize_performance(status, g_model, latent_dim, n_samples=25, save_models=True):\n","    # devise name\n","    gen_shape = g_model.output_shape\n","    name = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n","    # generate images\n","    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","    # normalize pixel values to the range [0,1]\n","    # X = (X - X.min()) / (X.max() - X.min())\n","    # plot real images\n","    square = int(np.sqrt(n_samples))\n","    for i in range(n_samples):\n","        pyplot.subplot(square, square, 1 + i)\n","        pyplot.axis('off')\n","        img = deprocess(X[i])\n","        img = np.clip(img, 0, 1)\n","        pyplot.imshow(img)\n","    # save plot to file\n","    filename1 = 'plot_%s.jpg' % (name)\n","    pyplot.savefig(filename1)\n","    pyplot.close()\n","    if save_models:\n","        # save the generator model\n","        filename2 = 'model_%s.h5' % (name)\n","        g_model.save(F\"models/{filename2}\")\n","        print('>Saved: %s and %s' % (filename1, filename2))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8BnOwvuWbT8","executionInfo":{"status":"ok","timestamp":1617799266378,"user_tz":-120,"elapsed":503,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["# load dataset\n","def load_real_samples(filename):\n","\t# load dataset\n","\tdata = load(filename)\n","\t# extract numpy array\n","\tX = data['arr_0']\n","\t# convert from ints to floats\n","\tX = X.astype('float32')\n","\t# scale from [0,255] to [-1,1]\n","\tX = (X - 127.5) / 127.5\n","\treturn X"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWKgLJZgWcj5","executionInfo":{"status":"ok","timestamp":1617799275420,"user_tz":-120,"elapsed":410,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels\n","\ty = ones((n_samples, 1))\n","\treturn X, y"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3MD1-MMVCiq","executionInfo":{"status":"ok","timestamp":1617798910054,"user_tz":-120,"elapsed":405,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def generate_latent_points(latent_dim, n_samples):\n","    # generate points in the latent space\n","    x_input = np.random.randn(latent_dim * n_samples)\n","    # reshape into a batch of inputs for the network\n","    x_input = x_input.reshape(n_samples, latent_dim)\n","    return x_input"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIEBcgvMVE6W","executionInfo":{"status":"ok","timestamp":1617798919933,"user_tz":-120,"elapsed":423,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def generate_fake_samples(generator, latent_dim, n_samples):\n","    # generate points in latent space\n","    x_input = generate_latent_points(latent_dim, n_samples)\n","    # predict outputs\n","    X = generator.predict(x_input)\n","    # create class labels\n","    y = -np.ones((n_samples, 1))\n","    return X, y"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFRm3mb4VHUg","executionInfo":{"status":"ok","timestamp":1617798927672,"user_tz":-120,"elapsed":499,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def update_fadein(models, step, n_steps):\n","    # calculate current alpha (linear from 0 to 1)\n","    alpha = step / float(n_steps - 1)\n","    # update the alpha for each model\n","    for model in models:\n","        for layer in model.layers:\n","            if isinstance(layer, WeightedSum):\n","                backend.set_value(layer.alpha, alpha)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"mDeyL2PNVJLn","executionInfo":{"status":"ok","timestamp":1617798935778,"user_tz":-120,"elapsed":394,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def show_images(generated_images, suffix=\"\"):\n","    n_images = len(generated_images)\n","    rows = 4\n","    cols = n_images // rows\n","\n","    plt.figure(figsize=(cols, rows))\n","    _, axs = plt.subplots(rows, cols)\n","    axs = axs.flatten()\n","    for im, ax in zip(generated_images, axs):\n","        img = deprocess(im)\n","        ax.imshow(img, cmap='gray')\n","    plt.savefig(F'prev_{suffix}.jpg')\n","    plt.show()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkcOn7QdVLMD","executionInfo":{"status":"ok","timestamp":1617798957378,"user_tz":-120,"elapsed":426,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["class PARAMETERS(object):\n","    n_blocks = 6\n","    latent_dim = 128\n","    n_batch = [16, 16, 16, 8, 4, 4]\n","    # n_epochs = [8, 8, 64, 64, 128, 128, 256]\n","    # n_epochs = [8, 8, 8, 8, 16, 32, 32]\n","    # n_epochs = [512, 1024, 2048, 5000, 5000, 5000, 5000]  # For AF faces, ap at 800k, like in the paper\n","    n_epochs = [5, 8, 8, 10, 10, 10]  # For faces, ap at 800k, like in the paper\n","    # n_epochs = [1, 1, 1, 1, 1, 1, 1]\n","    learning_rate = 0.0001\n","    adam_beta1 = 0.0\n","    adam_beta2 = 0.99\n","    adam_epsilon = 1e-8"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1nxYTyPVQdN","executionInfo":{"status":"ok","timestamp":1617799418908,"user_tz":-120,"elapsed":853,"user":{"displayName":"Linus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghdl6CLjCENKgMyGyEhcZdw6rkLvjwTgoT8moyc=s64","userId":"04895081854610684539"}}},"source":["def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, latent_dim, fadein=False):\n","    # calculate the number of batches per training epoch\n","    bat_per_epo = int(dataset.shape[0] / n_batch)\n","    # calculate the number of training iterations\n","    n_steps = bat_per_epo * n_epochs\n","    # calculate the size of half a batch of samples\n","    half_batch = int(n_batch / 2)\n","    # manually enumerate epochs\n","    t0 =time.time()\n","    for i in range(n_steps):\n","        # update alpha for all WeightedSum layers when fading in new blocks\n","        if fadein:\n","            update_fadein([g_model, d_model, gan_model], i, n_steps)\n","        # prepare real and fake samples\n","        X_real, y_real = generate_real_samples(dataset, half_batch)\n","        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","        # update discriminator model\n","        d_loss1 = d_model.train_on_batch(X_real, y_real)\n","        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n","        # update the generator via the discriminator's error\n","        z_input = generate_latent_points(latent_dim, n_batch)\n","        y_real2 = np.ones((n_batch, 1))\n","        g_loss = gan_model.train_on_batch(z_input, y_real2)\n","        # summarize loss on this batch\n","        print(F'\\r>{i + 1}/{n_steps}, d1=%.3f, d2=%.3f g=%.3f' % (d_loss1, d_loss2, g_loss), end=\"\")\n","        if time.time() - t0 > 30:\n","            # save preview of images every 30 s.\n","            t0 = time.time()\n","            try:\n","                summarize_performance('fresh_batch_preview', g_model, latent_dim, save_models=False)\n","            except:\n","                pass\n","    print(\"\")\n","\n","\n","# train the generator and discriminator\n","def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n","    # fit the baseline model\n","    g_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n","    # scale dataset to appropriate size\n","    gen_shape = g_normal.output_shape\n","    scaled_data = scale_dataset(dataset, gen_shape[1:])\n","    print('Scaled Data', gen_shape)\n","    # train normal or straight-through models\n","    train_epochs(g_normal, d_normal, gan_normal, scaled_dataset, e_norm[0], n_batch[0], latent_dim)\n","    try:\n","        summarize_performance('tuned', g_normal, latent_dim)\n","    except:\n","        pass\n","\n","    # process each level of growth\n","    for i in range(1, len(g_models)):\n","        # retrieve models for this level of growth\n","        [g_normal, g_fadein] = g_models[i]\n","        [d_normal, d_fadein] = d_models[i]\n","        [gan_normal, gan_fadein] = gan_models[i]\n","        # scale dataset to appropriate size\n","        gen_shape = g_normal.output_shape\n","        scaled_dataset = scale_dataset(dataset, gen_shape[1:])\n","        print('Scaled Data', gen_shape)\n","        # train fade-in models for next level of growth\n","        train_epochs(g_fadein, d_fadein, gan_fadein, dataset, n_epochs, e_fadein[i], n_batch[i], latent_dim, True)\n","        summarize_performance('faded', g_fadein, latent_dim)\n","        # train normal or straight-through models\n","        train_epochs(g_normal, d_normal, gan_normal, dataset, e_norm[i], n_batch[i], latent_dim)\n","        summarize_performance('tuned', g_normal, latent_dim)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YrVOewdXBBC"},"source":[""],"execution_count":null,"outputs":[]}]}